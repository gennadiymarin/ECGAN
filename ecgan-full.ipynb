{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b45b0e",
   "metadata": {
    "papermill": {
     "duration": 0.010133,
     "end_time": "2024-04-09T14:18:53.363334",
     "exception": false,
     "start_time": "2024-04-09T14:18:53.353201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beb45d",
   "metadata": {
    "papermill": {
     "duration": 0.010133,
     "end_time": "2024-04-09T14:18:53.383355",
     "exception": false,
     "start_time": "2024-04-09T14:18:53.373222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b09ad181",
   "metadata": {
    "papermill": {
     "duration": 0.009527,
     "end_time": "2024-04-09T14:18:53.402521",
     "exception": false,
     "start_time": "2024-04-09T14:18:53.392994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0746f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:18:53.422939Z",
     "iopub.status.busy": "2024-04-09T14:18:53.422559Z",
     "iopub.status.idle": "2024-04-09T14:18:57.405686Z",
     "shell.execute_reply": "2024-04-09T14:18:57.404484Z"
    },
    "papermill": {
     "duration": 3.996901,
     "end_time": "2024-04-09T14:18:57.408594",
     "exception": false,
     "start_time": "2024-04-09T14:18:53.411693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    n_epochs: int = 50\n",
    "    batch_size: int = 16\n",
    "    dataset: str = 'cityscapes'\n",
    "    data_path: str = '/kaggle/input/cityscapes-image-pairs/cityscapes_data'  # TODO\n",
    "    semantic_classes: int = 19\n",
    "    c_hidden: int = 64\n",
    "\n",
    "    loss_coefs: Dict = field(default_factory=lambda: {\n",
    "        'mma_G': 1,\n",
    "        'mma_D': 1,\n",
    "        'pix_contr': 1,\n",
    "        'L1': 1,\n",
    "        'sim': 1,\n",
    "        'perc': 10,\n",
    "        'discr_f': 10\n",
    "    })\n",
    "\n",
    "    lr = 0.0001\n",
    "    beta1 = 0.\n",
    "    beta2 = 0.999\n",
    "\n",
    "    project_name = 'ECGAN'\n",
    "\n",
    "    kernel_size = 3\n",
    "    padding = 1\n",
    "\n",
    "    LG_model_name = \"nvidia/segformer-b0-finetuned-cityscapes-768-768\"\n",
    "\n",
    "    cityscapes_classes = [\n",
    "        \"road\",\n",
    "        \"sidewalk\",\n",
    "        \"building\",\n",
    "        \"wall\",\n",
    "        \"fence\",\n",
    "        \"pole\",\n",
    "        \"traffic light\",\n",
    "        \"traffic sign\",\n",
    "        \"vegetation\",\n",
    "        \"terrain\",\n",
    "        \"sky\",\n",
    "        \"person\",\n",
    "        \"rider\",\n",
    "        \"car\",\n",
    "        \"truck\",\n",
    "        \"bus\",\n",
    "        \"train\",\n",
    "        \"motorcycle\",\n",
    "        \"bicycle\",\n",
    "    ]\n",
    "\n",
    "    cityscapes_palette = [\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [70, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb926db8",
   "metadata": {
    "papermill": {
     "duration": 0.008678,
     "end_time": "2024-04-09T14:18:57.426458",
     "exception": false,
     "start_time": "2024-04-09T14:18:57.417780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44004a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:18:57.448080Z",
     "iopub.status.busy": "2024-04-09T14:18:57.447253Z",
     "iopub.status.idle": "2024-04-09T14:19:01.248389Z",
     "shell.execute_reply": "2024-04-09T14:19:01.247238Z"
    },
    "papermill": {
     "duration": 3.814686,
     "end_time": "2024-04-09T14:19:01.251244",
     "exception": false,
     "start_time": "2024-04-09T14:18:57.436558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class CityScapesDataSet(Dataset):\n",
    "    def __init__(self, src_data: str, train=True):\n",
    "        self.path = src_data\n",
    "        self.path += '/train/' if train else '/val/'\n",
    "        self.length = 2975 if train else 500\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        img = self.transform(Image.open(self.path + str(item + 1) +'.jpg'))\n",
    "        return img[:,:,:256], img[:,:,256:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac881f",
   "metadata": {
    "papermill": {
     "duration": 0.008757,
     "end_time": "2024-04-09T14:19:01.269758",
     "exception": false,
     "start_time": "2024-04-09T14:19:01.261001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e842ba37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:19:01.290845Z",
     "iopub.status.busy": "2024-04-09T14:19:01.289966Z",
     "iopub.status.idle": "2024-04-09T14:19:01.589859Z",
     "shell.execute_reply": "2024-04-09T14:19:01.588365Z"
    },
    "papermill": {
     "duration": 0.314166,
     "end_time": "2024-04-09T14:19:01.593105",
     "exception": false,
     "start_time": "2024-04-09T14:19:01.278939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "def toRGB(x):\n",
    "    return ((x + 1) / 2 * 255).to(torch.uint8)\n",
    "\n",
    "def RGB2n(x, colors):  #x: batch x 3 x h x w, colors: 35 x 3\n",
    "    x1 = x.repeat(colors.shape[0], 1, 1, 1, 1).permute(1, 3, 4, 0, 2).int()  # B x h x w x 35 x 3\n",
    "    return torch.all(x1 == colors, dim=-1).permute(0, 3, 1, 2).float()  # B x 35 x H x W\n",
    "\n",
    "\n",
    "class Canny(nn.Module):\n",
    "    def __init__(self, t_low=100, t_high=200):\n",
    "        super().__init__()\n",
    "        self.t_low = t_low\n",
    "        self.t_high = t_high\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W) in (-1, 1)\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        B, C, H, W = x.shape\n",
    "        img = x.permute((0, 2, 3, 1))\n",
    "        img = toRGB(img).detach().cpu().numpy()\n",
    "        img = img.reshape(B * H, W, C)\n",
    "        edges = cv.Canny(img, self.t_low, self.t_high)\n",
    "        edges = torch.tensor(edges.reshape(B, H, W)).unsqueeze(1)\n",
    "        edges = edges.repeat(1, 3, 1, 1).to(device)\n",
    "        return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef10a34",
   "metadata": {
    "papermill": {
     "duration": 0.010224,
     "end_time": "2024-04-09T14:19:01.613343",
     "exception": false,
     "start_time": "2024-04-09T14:19:01.603119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c83d896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:19:01.635531Z",
     "iopub.status.busy": "2024-04-09T14:19:01.635082Z",
     "iopub.status.idle": "2024-04-09T14:19:18.112834Z",
     "shell.execute_reply": "2024-04-09T14:19:18.111536Z"
    },
    "papermill": {
     "duration": 16.492826,
     "end_time": "2024-04-09T14:19:18.115780",
     "exception": false,
     "start_time": "2024-04-09T14:19:01.622954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 14:19:04.924270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-09 14:19:04.924419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-09 14:19:05.114510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels_out)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_out)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, channels_out, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = self.projection(x_in) + x\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(channels_in, channels_out // 8),\n",
    "            ResBlock(channels_out // 8, channels_out // 4),\n",
    "            ResBlock(channels_out // 4, channels_out // 2),\n",
    "            ResBlock(channels_out // 2, channels_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.conv = spectral_norm(nn.Conv2d(channels_in, channels_out, 3, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_in, c_hidden, channels_out, n=3):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "\n",
    "        self.encoder = Encoder(channels_in, c_hidden)\n",
    "\n",
    "        convs_edge = []\n",
    "        convs_img = []\n",
    "\n",
    "        for _ in range(n):\n",
    "            convs_edge.append(ConvBlock(c_hidden, c_hidden // 2))\n",
    "            convs_img.append(ConvBlock(c_hidden, c_hidden // 2))\n",
    "            c_hidden //= 2\n",
    "\n",
    "        self.convs_edge = nn.ModuleList(convs_edge)\n",
    "        self.convs_img = nn.ModuleList(convs_img)\n",
    "\n",
    "        self.edge_final = nn.Sequential(\n",
    "            ConvBlock(c_hidden, channels_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.img_final = nn.Sequential(\n",
    "            ConvBlock(c_hidden, channels_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.canny = Canny()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "\n",
    "        f = self.encoder(x)\n",
    "\n",
    "        out_edge = f\n",
    "        out_img = f\n",
    "\n",
    "        for conv_e, conv_i in zip(self.convs_edge, self.convs_img):\n",
    "            out_edge = conv_e(out_edge)\n",
    "            out_img = conv_i(out_img)\n",
    "#             out_img += self.sigmoid(out_edge) * out_img\n",
    "            out_img = out_img + self.sigmoid(out_edge) * out_img\n",
    "\n",
    "        out_edge = self.edge_final(out_edge)\n",
    "        out_edge = self.canny(out_edge)  # TODO\n",
    "\n",
    "        out_img = self.img_final(out_img)\n",
    "#         out_img += self.sigmoid(out_edge) * out_img\n",
    "        out_img = out_img + self.sigmoid(out_edge) * out_img\n",
    "\n",
    "        return f, out_edge, out_img\n",
    "\n",
    "\n",
    "class SemanticPreserveModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels=config.c_hidden + config.semantic_classes + 6,\n",
    "                                             out_channels=config.semantic_classes,\n",
    "                                             kernel_size=config.kernel_size, padding=config.padding))\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(in_channels=config.semantic_classes,\n",
    "                                             out_channels=config.c_hidden + config.semantic_classes + 6,\n",
    "                                             kernel_size=config.kernel_size, padding=config.padding))\n",
    "\n",
    "        self.final = spectral_norm(nn.Conv2d(in_channels=config.c_hidden + config.semantic_classes + 6,\n",
    "                                             out_channels=3,\n",
    "                                             kernel_size=config.kernel_size, padding=config.padding))\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.conv1(x)\n",
    "\n",
    "        out = self.avg_pool(identity)\n",
    "        out = self.sigmoid(self.avg_pool(out))\n",
    "        out = out * identity + identity\n",
    "\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.tanh(self.final(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 256 -> 128\n",
    "            self._downsample_block(3 + 19, 64),  # 128 -> 64\n",
    "            self._downsample_block(64, 128),  # 64 -> 32\n",
    "            self._downsample_block(128, 256),  # 32 -> 16\n",
    "            self._downsample_block(256, 512),  # 16 -> 8\n",
    "            nn.Conv2d(512, 64, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(64 * 8 * 8, 128)),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1))\n",
    "\n",
    "    def _downsample_block(self, input_channels, output_channels):\n",
    "        return nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1, stride=2)),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x, y):  #x: B x 3 x H x W, y: B x 19 x H x W\n",
    "        x1 = torch.concat([x, y], dim=1)  # B x C + N x H x W\n",
    "        x1 = self.layers(x1).reshape(x1.shape[0], -1)\n",
    "        x1 = self.fc(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "class LabelGenerator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LabelGenerator, self).__init__()\n",
    "\n",
    "        self.processor = SegformerImageProcessor(do_resize=False)\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(config.LG_model_name)\n",
    "        self.device = config.device\n",
    "        self.palette = config.cityscapes_palette\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        pixel_values = self.processor(toRGB(imgs), return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values)\n",
    "\n",
    "        predicted_segmentation_maps = self.processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                        target_sizes=[imgs.shape[2:] for\n",
    "                                                                                                      _ in range(\n",
    "                                                                                                imgs.shape[0])])\n",
    "        predicted_segmentation_maps = torch.stack(predicted_segmentation_maps, dim=0).cpu().numpy()  # B x H x W\n",
    "\n",
    "        color_segs = np.zeros((predicted_segmentation_maps.shape[0],\n",
    "                               predicted_segmentation_maps.shape[1],\n",
    "                               predicted_segmentation_maps.shape[2], 3), dtype=np.uint8)  # B x H x W x 3\n",
    "\n",
    "        palette = np.array(self.palette)\n",
    "        for label, color in enumerate(palette):\n",
    "            color_segs[predicted_segmentation_maps == label, :] = color\n",
    "\n",
    "        color_segs = torch.tensor(color_segs).permute(0, 3, 1, 2).to(self.device)  # B x 3 x H x W\n",
    "\n",
    "        return color_segs  # RGB non-normalized\n",
    "\n",
    "\n",
    "\n",
    "class ECGAN(nn.Module):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.generator = Generator(config.semantic_classes, config.c_hidden, 3)\n",
    "        self.discriminator = Discriminator(config)\n",
    "        self.semantic_preserving_module = SemanticPreserveModule(config)\n",
    "        self.label_generator = LabelGenerator(config)\n",
    "        self.canny = Canny()\n",
    "\n",
    "\n",
    "    def forward(self, s, img):\n",
    "        f, out_edge, out_img1 = self.generator(s)\n",
    "\n",
    "        out_img2 = self.semantic_preserving_module(torch.cat([s, f, out_edge, out_img1], dim=1))\n",
    "\n",
    "        labels = self.label_generator(out_img2)\n",
    "\n",
    "        return f, out_edge, out_img1, out_img2, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb6ece",
   "metadata": {
    "papermill": {
     "duration": 0.009446,
     "end_time": "2024-04-09T14:19:18.134977",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.125531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9f48b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:19:18.156930Z",
     "iopub.status.busy": "2024-04-09T14:19:18.155975Z",
     "iopub.status.idle": "2024-04-09T14:19:18.192081Z",
     "shell.execute_reply": "2024-04-09T14:19:18.190711Z"
    },
    "papermill": {
     "duration": 0.050686,
     "end_time": "2024-04-09T14:19:18.195307",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.144621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "def pixel_contrastive_loss(img_seg, f, labels):\n",
    "    '''\n",
    "    :param img_seg: (B, 3, H, W)\n",
    "    :param f: (B, C, H, W)\n",
    "    :param labels:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    img_seg = torch.permute(\n",
    "        img_seg, (0, 2, 3, 1)).flatten(end_dim=-2)  # (-1 , 3)\n",
    "    f = torch.permute(f, (0, 2, 3, 1)).flatten(end_dim=-2)  # (-1 , C)\n",
    "\n",
    "    for label in labels:\n",
    "        mask = torch.all(img_seg == label, dim=-1)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        pos = np.random.choice(len(f[mask]), min(\n",
    "            10000, len(f[mask])), replace=False)\n",
    "        neg = np.random.choice(len(f[~mask]), min(\n",
    "            10000, len(f[~mask])), replace=False)\n",
    "\n",
    "        pos_batch = f[mask][pos] / \\\n",
    "                    (f[mask][pos] ** 2).sum(dim=1).sqrt().reshape(-1, 1)\n",
    "        neg_batch = f[~mask][neg] / \\\n",
    "                    (f[~mask][neg] ** 2).sum(dim=1).sqrt().reshape(-1, 1)\n",
    "\n",
    "        loss = torch.exp(pos_batch @ pos_batch.T)\n",
    "        sum_neg = torch.exp(pos_batch @ neg_batch.T).sum(dim=1)\n",
    "        loss = -torch.log(loss / (loss + sum_neg)).sum() / \\\n",
    "               mask.sum() / pos_batch.size(0)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def reconstructive_loss(img_true, img_pred):\n",
    "    '''\n",
    "    :param img_true: (B, 3, H, W)\n",
    "    :param img_pred: (B, 3, H, W)\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    return abs(img_true - img_pred).sum()\n",
    "\n",
    "\n",
    "def generator_mma_loss(edge_fake_logits, img_fake1_logits, img_fake2_logits, lmbd=2):\n",
    "    edge_fake_loss = F.binary_cross_entropy_with_logits(edge_fake_logits, torch.ones_like(edge_fake_logits))\n",
    "    img_fake_loss1 = F.binary_cross_entropy_with_logits(img_fake1_logits, torch.ones_like(img_fake1_logits))\n",
    "    img_fake_loss2 = F.binary_cross_entropy_with_logits(img_fake2_logits, torch.ones_like(img_fake2_logits))\n",
    "    return lmbd * img_fake_loss2 + img_fake_loss1 + edge_fake_loss\n",
    "\n",
    "\n",
    "def discr_mma_loss(edge_real_logits, img_real_logits, edge_fake_logits, img_fake1_logits, img_fake2_logits, lmbd=2):\n",
    "    edge_real_loss = F.binary_cross_entropy_with_logits(edge_real_logits, torch.ones_like(edge_real_logits))\n",
    "    edge_fake_loss = F.binary_cross_entropy_with_logits(edge_fake_logits.detach(), torch.zeros_like(edge_fake_logits))\n",
    "\n",
    "    img_real_loss = F.binary_cross_entropy_with_logits(img_real_logits, torch.ones_like(img_real_logits))\n",
    "    img_fake_loss1 = F.binary_cross_entropy_with_logits(img_fake1_logits.detach(), torch.zeros_like(img_fake1_logits))\n",
    "    img_fake_loss2 = F.binary_cross_entropy_with_logits(img_fake2_logits.detach(), torch.zeros_like(img_fake2_logits))\n",
    "\n",
    "    return (lmbd + 1) * img_real_loss + lmbd * img_fake_loss2 + img_fake_loss1 + \\\n",
    "        edge_real_loss + edge_fake_loss\n",
    "\n",
    "\n",
    "# https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49\n",
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks = []\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = 0.0\n",
    "        x = input\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            loss += torch.nn.functional.l1_loss(x, y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def similarity_loss(x, y):  # N x H x W\n",
    "    x1 = x.reshape(x.shape[0], -1)  # N x M     M = HW\n",
    "    x_s = x1 @ x1.T  # M x M\n",
    "\n",
    "    y1 = y.reshape(y.shape[0], -1)\n",
    "    y_s = y1 @ y1.T\n",
    "\n",
    "    m = x_s.shape[0]\n",
    "\n",
    "    return -1 / (m * m) * (y_s * x_s.log() + (1 - y_s) * (1 - x_s).log()).sum()\n",
    "\n",
    "\n",
    "def disc_feature_loss(x, y):\n",
    "    return F.l1_loss(x, y)\n",
    "\n",
    "\n",
    "class GANLossFactory:\n",
    "    def __init__(self, config):\n",
    "        self.perc = VGGPerceptualLoss().to(config.device)\n",
    "        self._coefs = config.loss_coefs\n",
    "        self._losses = {\n",
    "            'mma_G': generator_mma_loss,\n",
    "            'mma_D': discr_mma_loss,\n",
    "            'pix_contr': pixel_contrastive_loss,\n",
    "            'L1': reconstructive_loss,\n",
    "            'sim': similarity_loss,\n",
    "            'perc': self.perc,\n",
    "            'discr_f': disc_feature_loss,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return lambda *args, **kwargs: self._coefs[key] * self._losses[key](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3b92e",
   "metadata": {
    "papermill": {
     "duration": 0.009389,
     "end_time": "2024-04-09T14:19:18.214222",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.204833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c851fe21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:19:18.235333Z",
     "iopub.status.busy": "2024-04-09T14:19:18.234877Z",
     "iopub.status.idle": "2024-04-09T14:19:18.263674Z",
     "shell.execute_reply": "2024-04-09T14:19:18.262224Z"
    },
    "papermill": {
     "duration": 0.042879,
     "end_time": "2024-04-09T14:19:18.266727",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.223848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# import wandb\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.device = self.config.device\n",
    "        self.model = ECGAN(self.config).to(self.device)\n",
    "        self.optimizers = {\n",
    "            'G': torch.optim.Adam(self.model.generator.parameters(), lr=self.config.lr,\n",
    "                                  betas=(self.config.beta1, self.config.beta2)),\n",
    "            'D': torch.optim.Adam(self.model.discriminator.parameters(), lr=self.config.lr,\n",
    "                                  betas=(self.config.beta1, self.config.beta2)),\n",
    "        }\n",
    "\n",
    "        dataset = CityScapesDataSet(self.config.data_path, train=True)\n",
    "        self.loader = DataLoader(dataset, batch_size=self.config.batch_size,\n",
    "                                 shuffle=True, num_workers=0)\n",
    "\n",
    "        self.losses = GANLossFactory(self.config)\n",
    "        self.labels = torch.tensor(config.cityscapes_palette, device=self.device)\n",
    "        self.epoch = 0\n",
    "\n",
    "#         wandb.init(project=config.project_name)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        for img, img_seg in tqdm(self.loader, desc='Training'):\n",
    "            img = img.to(self.device)\n",
    "            img_seg = img_seg.to(self.device)\n",
    "\n",
    "            s = RGB2n(img_seg, self.labels)\n",
    "            \n",
    "            loss_D = self.update_D(img, s)\n",
    "            loss_G = self.update_G(img, img_seg, s)\n",
    "\n",
    "        # self.log_images()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self):\n",
    "        img, img_seg = self.log_images\n",
    "        s = RGB2n(img_seg, self.labels)\n",
    "\n",
    "        self.model.eval()\n",
    "        f, out_edge, out_img1, out_img2, pred_labels = self.model(s, img)\n",
    "        img_edge = self.model.canny(img)\n",
    "\n",
    "#         images = wandb.Image([img, out_img1, out_img2], caption='images')\n",
    "#         edges = wandb.Image([img_edge, out_edge], caption='edges')\n",
    "\n",
    "#         wandb.log({\"examples\": images, \"edges\": edges})\n",
    "\n",
    "    def update_D(self, img, s):\n",
    "        self.optimizers['D'].zero_grad()\n",
    "\n",
    "        img_edge = self.model.canny(img)\n",
    "        f, out_edge, out_img1, out_img2, pred_labels = self.model(s, img)\n",
    "\n",
    "        edge_real_logits = self.model.discriminator(img_edge, s)\n",
    "        edge_fake_logits = self.model.discriminator(out_edge, s)\n",
    "        img_real_logits = self.model.discriminator(img, s)\n",
    "        img_fake1_logits = self.model.discriminator(out_img1, s)\n",
    "        img_fake2_logits = self.model.discriminator(out_img2, s)\n",
    "\n",
    "        loss = self.losses['mma_D'](edge_real_logits, img_real_logits, edge_fake_logits, img_fake1_logits,\n",
    "                                    img_fake2_logits)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizers['D'].step()\n",
    "\n",
    "    def update_G(self, img, img_seg, s):\n",
    "        self.optimizers['G'].zero_grad()\n",
    "        f, out_edge, out_img1, out_img2, pred_labels = self.model(s, img)\n",
    "        s_fake = RGB2n(pred_labels, self.labels)\n",
    "        img_edge = self.model.canny(img)\n",
    "\n",
    "        edge_real_logits = self.model.discriminator(img_edge, s).detach()\n",
    "        edge_fake_logits = self.model.discriminator(out_edge, s)\n",
    "        img_real_logits = self.model.discriminator(img, s).detach()\n",
    "        img_fake1_logits = self.model.discriminator(out_img1, s)\n",
    "        img_fake2_logits = self.model.discriminator(out_img2, s)\n",
    "\n",
    "        loss = self.losses['mma_G'](edge_fake_logits, img_fake1_logits, img_fake2_logits) \\\n",
    "               + self.losses['pix_contr'](img_seg, f, self.labels) \\\n",
    "               + self.losses['L1'](img, out_img1) \\\n",
    "               + self.losses['sim'](s, s_fake) \\\n",
    "               + self.losses['perc'](img_edge.float(), out_edge.float()) \\\n",
    "               + self.losses['perc'](img, out_img1) \\\n",
    "               + self.losses['perc'](img, out_img2) \\\n",
    "               + self.losses['discr_f'](edge_real_logits, edge_fake_logits) \\\n",
    "               + self.losses['discr_f'](img_real_logits, img_fake1_logits) \\\n",
    "               + self.losses['discr_f'](img_real_logits, img_fake2_logits)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizers['G'].step()\n",
    "\n",
    "    def train(self):\n",
    "        for self.epoch in range(1, self.config.n_epochs + 1):\n",
    "            self.train_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e94529",
   "metadata": {
    "papermill": {
     "duration": 0.008924,
     "end_time": "2024-04-09T14:19:18.285116",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.276192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f867d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T14:19:18.305439Z",
     "iopub.status.busy": "2024-04-09T14:19:18.305053Z",
     "iopub.status.idle": "2024-04-09T14:19:18.309993Z",
     "shell.execute_reply": "2024-04-09T14:19:18.308325Z"
    },
    "papermill": {
     "duration": 0.018976,
     "end_time": "2024-04-09T14:19:18.313292",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.294316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = CFG()\n",
    "# trainer = Trainer(config)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324078d",
   "metadata": {
    "papermill": {
     "duration": 0.008799,
     "end_time": "2024-04-09T14:19:18.331810",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.323011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3e8a4",
   "metadata": {
    "papermill": {
     "duration": 0.00936,
     "end_time": "2024-04-09T14:19:18.350429",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.341069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0cd27",
   "metadata": {
    "papermill": {
     "duration": 0.008924,
     "end_time": "2024-04-09T14:19:18.369605",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.360681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01b877",
   "metadata": {
    "papermill": {
     "duration": 0.009067,
     "end_time": "2024-04-09T14:19:18.387831",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.378764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6630f",
   "metadata": {
    "papermill": {
     "duration": 0.009505,
     "end_time": "2024-04-09T14:19:18.407258",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.397753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f03bb",
   "metadata": {
    "papermill": {
     "duration": 0.008785,
     "end_time": "2024-04-09T14:19:18.425318",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.416533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c260f",
   "metadata": {
    "papermill": {
     "duration": 0.009003,
     "end_time": "2024-04-09T14:19:18.443625",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.434622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3ee06",
   "metadata": {
    "papermill": {
     "duration": 0.010004,
     "end_time": "2024-04-09T14:19:18.463858",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.453854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb73f0",
   "metadata": {
    "papermill": {
     "duration": 0.008992,
     "end_time": "2024-04-09T14:19:18.482760",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.473768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5784f5",
   "metadata": {
    "papermill": {
     "duration": 0.009404,
     "end_time": "2024-04-09T14:19:18.502330",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.492926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67c5a0",
   "metadata": {
    "papermill": {
     "duration": 0.009895,
     "end_time": "2024-04-09T14:19:18.521830",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.511935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc499e",
   "metadata": {
    "papermill": {
     "duration": 0.009236,
     "end_time": "2024-04-09T14:19:18.543999",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.534763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e602bd9",
   "metadata": {
    "papermill": {
     "duration": 0.008863,
     "end_time": "2024-04-09T14:19:18.562550",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.553687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2aebaf",
   "metadata": {
    "papermill": {
     "duration": 0.008751,
     "end_time": "2024-04-09T14:19:18.580447",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.571696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648e001",
   "metadata": {
    "papermill": {
     "duration": 0.008763,
     "end_time": "2024-04-09T14:19:18.598408",
     "exception": false,
     "start_time": "2024-04-09T14:19:18.589645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 22655,
     "sourceId": 29047,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.187242,
   "end_time": "2024-04-09T14:19:21.405004",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-09T14:18:50.217762",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
